{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SkinAnaliticAI, Skin Cancer Detection with AI Deep Learning\n",
    "\n",
    "## __Evaluation of Harvard Dataset with different AI classiffication techniques using FastClassAI papeline__\n",
    "Author: __Pawel Rosikiewicz__   \n",
    "prosikiewicz@gmail.com      \n",
    "License: __MIT__    \n",
    "ttps://opensource.org/licenses/MIT        \n",
    "Copyright (C) 2021.01.30 Pawel Rosikiewicz        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### standard imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # allow changing, and navigating files and folders, \n",
    "import sys\n",
    "import shutil\n",
    "import re # module to use regular expressions, \n",
    "import glob # lists names in folders that match Unix shell patterns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup basedir\n",
    "basedir = os.path.dirname(os.getcwd())\n",
    "os.chdir(basedir)\n",
    "sys.path.append(basedir)\n",
    "\n",
    "# set up paths for the project\n",
    "PATH_raw = os.path.join(basedir, \"data/raw\")\n",
    "PATH_interim = os.path.join(basedir, \"data/interim\")\n",
    "PATH_models = os.path.join(basedir, \"models\")\n",
    "PATH_interim_dataset_summary_tables = os.path.join(PATH_interim, \"dataset_summary_tables\") # create in that notebook,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load functions,\n",
    "from src.utils.feature_extraction_tools import encode_images\n",
    "\n",
    "# load configs\n",
    "from src.configs.project_configs import CLASS_DESCRIPTION # information on each class, including descriptive class name and diegnostic description - used to help wiht the project\n",
    "from src.configs.tfhub_configs import TFHUB_MODELS # names of TF hub modules that I presenlected for featuress extraction with all relevant info,\n",
    "from src.configs.dataset_configs import DATASET_CONFIGS # names created for clases, assigned to original one, and colors assigned to these classes\n",
    "from src.configs.dataset_configs import CLASS_LABELS_CONFIGS # names created for clases, assigned to original one, and colors assigned to these classes\n",
    "from src.configs.dataset_configs import DROPOUT_VALUE # str, special value to indicate samples to remoce in class labels\n",
    "from src.configs.config_functions import DEFINE_DATASETS # function that creates datasunbsets collections for one dataset (custome made for that project)\n",
    "\n",
    "# set project variables\n",
    "PROJECT_NAME                      = \"SkinAnaliticAI_Harvard_dataset_evaluation\" # \n",
    "DATASET_NAME                      = \"HAM10000\"  # name used in config files to identify all info on that dataset variant\n",
    "DATASET_VARIANTS                  = DATASET_CONFIGS[DATASET_NAME][\"labels\"] # class labels that will be used, SORT_FILES_WITH   must be included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- 0 - Extracting features from: Cancer_Detection_And_Classification\n",
      "\n",
      " ................................................\n",
      "  - 0/0 module:          MobileNet_v2\n",
      "  - 0/0 filename or url: imagenet_mobilenet_v2_100_224_feature_vector_2\n",
      "  - 0/0 RGB image size : (224, 224)\n",
      "  - 0/0 datset subsets : ['train_05', 'train_02', 'valid_01', 'train_03', 'train_04', 'test_01', 'train_01', 'train_06', 'valid_02', 'train_07', 'test_02']\n",
      "  - Cataloging subsets, then extracting features from all images\n",
      "  - Important: Each subset will be saved as one matrix\n",
      "\n",
      "\n",
      "Found 744 images belonging to 7 classes.\n",
      "Found 742 images belonging to 7 classes.\n",
      "Found 740 images belonging to 7 classes.\n",
      "Found 742 images belonging to 7 classes.\n",
      "Found 744 images belonging to 7 classes.\n",
      "Found 367 images belonging to 7 classes.\n",
      "Found 742 images belonging to 7 classes.\n",
      "Found 738 images belonging to 7 classes.\n",
      "Found 741 images belonging to 7 classes.\n",
      "Found 751 images belonging to 7 classes.\n",
      "Found 367 images belonging to 7 classes.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "\n",
      " ................................................\n",
      "  - 0/1 module:          BiT_M_Resnet101\n",
      "  - 0/1 filename or url: bit_m-r101x1_1\n",
      "  - 0/1 RGB image size : (224, 224)\n",
      "  - 0/1 datset subsets : ['train_05', 'train_02', 'valid_01', 'train_03', 'train_04', 'test_01', 'train_01', 'train_06', 'valid_02', 'train_07', 'test_02']\n",
      "  - Cataloging subsets, then extracting features from all images\n",
      "  - Important: Each subset will be saved as one matrix\n",
      "\n",
      "\n",
      "Found 744 images belonging to 7 classes.\n",
      "Found 742 images belonging to 7 classes.\n",
      "Found 740 images belonging to 7 classes.\n",
      "Found 742 images belonging to 7 classes.\n",
      "Found 744 images belonging to 7 classes.\n",
      "Found 367 images belonging to 7 classes.\n",
      "Found 742 images belonging to 7 classes.\n",
      "Found 738 images belonging to 7 classes.\n",
      "Found 741 images belonging to 7 classes.\n",
      "Found 751 images belonging to 7 classes.\n",
      "Found 367 images belonging to 7 classes.\n",
      "\n",
      "- 1 - Extracting features from: Cancer_Risk_Groups\n",
      "\n",
      " ................................................\n",
      "  - 1/0 module:          MobileNet_v2\n",
      "  - 1/0 filename or url: imagenet_mobilenet_v2_100_224_feature_vector_2\n",
      "  - 1/0 RGB image size : (224, 224)\n",
      "  - 1/0 datset subsets : ['train_05', 'train_02', 'valid_01', 'train_03', 'train_04', 'test_01', 'train_01', 'train_06', 'valid_02', 'train_07', 'test_02']\n",
      "  - Cataloging subsets, then extracting features from all images\n",
      "  - Important: Each subset will be saved as one matrix\n",
      "\n",
      "\n",
      "Found 743 images belonging to 3 classes.\n",
      "Found 742 images belonging to 3 classes.\n",
      "Found 741 images belonging to 3 classes.\n",
      "Found 742 images belonging to 3 classes.\n",
      "Found 743 images belonging to 3 classes.\n",
      "Found 369 images belonging to 3 classes.\n",
      "Found 741 images belonging to 3 classes.\n",
      "Found 740 images belonging to 3 classes.\n",
      "Found 741 images belonging to 3 classes.\n",
      "Found 746 images belonging to 3 classes.\n",
      "Found 370 images belonging to 3 classes.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "\n",
      " ................................................\n",
      "  - 1/1 module:          BiT_M_Resnet101\n",
      "  - 1/1 filename or url: bit_m-r101x1_1\n",
      "  - 1/1 RGB image size : (224, 224)\n",
      "  - 1/1 datset subsets : ['train_05', 'train_02', 'valid_01', 'train_03', 'train_04', 'test_01', 'train_01', 'train_06', 'valid_02', 'train_07', 'test_02']\n",
      "  - Cataloging subsets, then extracting features from all images\n",
      "  - Important: Each subset will be saved as one matrix\n",
      "\n",
      "\n",
      "Found 743 images belonging to 3 classes.\n",
      "Found 742 images belonging to 3 classes.\n",
      "Found 741 images belonging to 3 classes.\n",
      "Found 742 images belonging to 3 classes.\n",
      "Found 743 images belonging to 3 classes.\n",
      "Found 369 images belonging to 3 classes.\n",
      "Found 741 images belonging to 3 classes.\n",
      "Found 740 images belonging to 3 classes.\n",
      "Found 741 images belonging to 3 classes.\n",
      "Found 746 images belonging to 3 classes.\n",
      "Found 370 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# preset values \n",
    "generator_batch_size = 3000 # no more then 3000 images will be taken,  but we expect no more then 2000 in that tassk. \n",
    "use_url = \"no\" # the script is adapted only to use sys.path, but configs carries url's and ulr can be used with feature extraction function \n",
    "\n",
    "# extract features from images in each dataset varinat using one or more tf hub modules, \n",
    "for dv_i, dataset_variant in enumerate(DATASET_VARIANTS):\n",
    "    \n",
    "    print(f\"\\n- {dv_i} - Extracting features from: {dataset_variant}\")\n",
    "    \n",
    "    # find names off train/valid/test subsets in dataset folder,\n",
    "    os.chdir(os.path.join(PATH_interim, f\"{DATASET_NAME}__{dataset_variant}\"))\n",
    "    subset_names_to_encode = []\n",
    "    for file in glob.glob(f\"[train|valid|test]*\"):\n",
    "        subset_names_to_encode.append(file)\n",
    "        \n",
    "        \n",
    "    # Create lists with info required for feture extraction from images \n",
    "    'this step is super usefull when many models is used for feature extraction'\n",
    "    tfmodules          = list(TFHUB_MODELS.keys()) # names of tf hub models used \n",
    "    module_names       = [TFHUB_MODELS[x]['module_name'] for x in tfmodules]\n",
    "    module_file_names  = [TFHUB_MODELS[x]['file_name'] for x in tfmodules]\n",
    "    img_imput_size     = [TFHUB_MODELS[x]['input_size'] for x in tfmodules]\n",
    "\n",
    "    \n",
    "    # extract features, from images from each subset, and store them togther as one batch array,  \n",
    "    for i, (one_module_name, one_module_file_name, one_img_input_size) in enumerate(zip(module_names, module_file_names, img_imput_size)):\n",
    "        '''\n",
    "            all data subsets found in load_dir will be encoded automatically, \n",
    "            - logfile will be created for a given datasets\n",
    "            - batch_labels csv file and npz file with encoded features will be created for \n",
    "            each data subset will have:\n",
    "            - \n",
    "        '''\n",
    "        print(\"\\n ................................................\")\n",
    "        print(f\"  - {dv_i}/{i} module:          {one_module_name}\")\n",
    "        print(f\"  - {dv_i}/{i} filename or url: {one_module_file_name}\")\n",
    "        print(f\"  - {dv_i}/{i} RGB image size : {one_img_input_size}\")\n",
    "        print(f\"  - {dv_i}/{i} datset subsets : {subset_names_to_encode}\")\n",
    "        print(f\"  - Cataloging subsets, then extracting features from all images\")\n",
    "        print(f\"  - Important: Each subset will be saved as one matrix\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # I am using modules saved in computer memory, thus I need to build fiull path to them, \n",
    "        if use_url==\"no\":\n",
    "            one_module_full_path = os.path.join(PATH_models, one_module_file_name)\n",
    "        else:\n",
    "            one_module_full_path = one_module_file_name # here I am using module url, (no path)\n",
    " \n",
    "        # extract features    \n",
    "        encode_images(\n",
    "\n",
    "            # .. dastaset name & directories, \n",
    "            dataset_name     = f\"{DATASET_NAME}__{dataset_variant}\",# name used when saving encoded files, logfiles and other things, related to encoding, \n",
    "            subset_names     = subset_names_to_encode,# list, ust names of files in the load_dir, if any, \n",
    "            load_dir         = os.path.join(PATH_interim, f\"{DATASET_NAME}__{dataset_variant}\"),   # full path to input data, ie. file folder with either folders with images names after class names, or folders with subsetnames, and folders names after each class in them, \n",
    "            save_dir         = os.path.join(PATH_interim, f\"{DATASET_NAME}__{dataset_variant}__extracted_features\"), # all new files, will be saved as one batch, with logfile, if None, load_dir will be used, \n",
    "\n",
    "            # .. encoding module parameters, \n",
    "            module_name          = one_module_name, # name used when saving files\n",
    "            module_location      = one_module_full_path, # full path to a given module, or url, \n",
    "            img_target_size      = one_img_input_size, # image resolution in pixels, \n",
    "            generator_batch_size = generator_batch_size, # must be larger or equal to in size of the largest subset\n",
    "            generator_shuffle    = False, \n",
    "\n",
    "            # .. other, \n",
    "            save_files       = True,\n",
    "            verbose          = False                            \n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
